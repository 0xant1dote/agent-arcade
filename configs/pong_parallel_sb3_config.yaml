# Algorithm and Environment
algo: "DQN"
env: "ALE/Pong-v5"

# Core Training Parameters
total_timesteps: 2000000
learning_rate: 0.00025
buffer_size: 50000
learning_starts: 10000
batch_size: 32
train_freq: 4
gradient_steps: 1
tau: 1.0
max_grad_norm: 10

# Exploration Strategy
exploration_initial_eps: 1.0
exploration_fraction: 0.3
exploration_final_eps: 0.05

# Network Architecture (Nature DQN)
policy_kwargs:
  net_arch: []
  normalize_images: true
  features_extractor_kwargs:
    features_dim: 512

# Network Parameters
gamma: 0.99
target_update_interval: 1000
frame_stack: 4

# Environment Processing
scale_rewards: false
normalize_frames: true
terminal_on_life_loss: true
episode_life: true
clip_reward: false
screen_size: 84
frame_skip: 4
noop_max: 30

# Training Monitoring
train_log_interval: 100
video_interval: 10000
video_length: 200
checkpoint_interval: 10000
eval_freq: 10000
n_eval_episodes: 5

# Performance Settings
n_envs: 4
optimize_memory_usage: true
device: "auto"
verbose: 1

# Competition Settings
difficulty: 0
mode: 0 